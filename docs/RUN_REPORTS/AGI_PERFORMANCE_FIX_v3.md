# AGI Performance Fix v3 ‚Äî Diagnostic & Solutions

**Date :** 2025-11-11  
**Probl√®me :** 4h pour 1 it√©ration AGI (inacceptable)  
**Cause racine :** ~340 simulations CA par candidat (boucles Python pures)

---

## üî¥ DIAGNOSTIC

### Bottleneck Identifi√©

**Dans `MemoryExplorer.evaluate_candidate()` (ligne 170-172) :**

```python
# Chaque candidat subit :
capacity_result = compute_memory_capacity(
    rule_func, grid_size=(16,16), n_patterns=5, steps=30
)  # ‚Üí 5 patterns √ó 30 steps = 150 simulations

robustness_result = compute_robustness_to_noise(
    rule_func, grid_size=(16,16), noise_level=0.1, n_trials=3, steps=30
)  # ‚Üí 3 trials √ó 30 steps = 90 simulations

basin_result = compute_basin_size(
    rule_func, grid_size=(16,16), n_samples=5, steps=20
)  # ‚Üí 5 samples √ó 20 steps = 100 simulations
```

**Total : ~340 simulations CA par candidat**

**Avec batch_size=4 :** 4 √ó 340 = **1360 simulations/it√©ration**

**Avec Python pur (pas NumPy vectoris√©) :** ~10-15s de simulation par it√©ration ‚Üí 4h

---

## üí° SOLUTIONS (3 Niveaux)

### üèÉ NIVEAU 1 : Mode Fast (URGENT)

**Objectif :** 50 it√©rations en <30 minutes

**Changements :**
1. **√âvaluation rapide** par d√©faut dans boucle AGI :
   - `grid_size=(16,16)` (vs 32√ó32)
   - `steps=50` (vs 120)
   - `n_patterns=3`, `n_trials=2`, `n_samples=3`
   - **R√©duction : ~340 ‚Üí ~120 simulations/candidat**

2. **Audit mode** (lourd) r√©serv√© aux top candidats :
   - Appliqu√© seulement aux r√®gles promues en HoF
   - Grilles 32√ó32, 64√ó64, stress-tests complets

**Impl√©mentation :**
```python
# Dans closed_loop_agi.py, run_one_iteration()
results = self.explorer.explore_batch(
    candidates,
    grid_size=(16, 16),  # Fast mode
    steps=50,            # Fast mode
    seed=self.config['evaluation_seed'],
    compute_functional=True,
    fast_mode=True       # Nouveau flag
)

# Puis audit lourd pour les promus
for promoted_rule in hof_added:
    audit_results = self.explorer.stress_test(
        promoted_rule, 
        grid_sizes=[(32,32), (64,64)],
        noise_levels=[0.1, 0.2, 0.3]
    )
```

**Gain estim√© :** 4h ‚Üí ~20-30 minutes pour 50 it√©rations

---

### ‚ö° NIVEAU 2 : Vectorisation NumPy (MOYEN TERME)

**Objectif :** 50 it√©rations en <10 minutes

**Probl√®me actuel :**
```python
# Dans _create_rule_function(), boucles Python imbriqu√©es :
for i in range(h):
    for j in range(w):
        neighbors = sum(grid[(i+di)%h, (j+dj)%w] ...)
        # Tr√®s lent
```

**Solution :**
```python
# Vectorisation avec scipy.signal.convolve2d
from scipy.signal import convolve2d

def step_ca_vectorized(grid, born_set, survive_set):
    kernel = np.array([[1,1,1], [1,0,1], [1,1,1]])
    neighbor_count = convolve2d(grid, kernel, mode='same', boundary='wrap')
    
    # Masques bool√©ens (vectoris√©s)
    alive = grid == 1
    dead = grid == 0
    
    born_mask = np.isin(neighbor_count, list(born_set))
    survive_mask = np.isin(neighbor_count, list(survive_set))
    
    new_grid = np.zeros_like(grid)
    new_grid[alive & survive_mask] = 1
    new_grid[dead & born_mask] = 1
    
    return new_grid
```

**Gain estim√© :** 10-20√ó plus rapide (20-30 min ‚Üí 2-5 min pour 50 it√©rations)

---

### üöÄ NIVEAU 3 : Numba JIT (OPTIONNEL)

**Objectif :** 50 it√©rations en <5 minutes

**Solution :**
```python
from numba import jit

@jit(nopython=True)
def step_ca_numba(grid, born_array, survive_array):
    # Compilation JIT pour boucles Python
    # Gain : 50-100√ó vs Python pur
```

**Gain estim√© :** ~50√ó plus rapide (20 min ‚Üí <5 min)

---

## üéØ PLAN D'ACTION IMM√âDIAT

### Phase 1 : Mode Fast (Ce Soir, 30 minutes)

1. ‚úÖ Ajouter flag `fast_mode` dans `evaluate_candidate()`
2. ‚úÖ R√©duire : `n_patterns=3`, `n_trials=2`, `n_samples=3`, `steps=50`
3. ‚úÖ Tester : 10 it√©rations en <5 minutes
4. ‚úÖ Lancer : 50 it√©rations overnight

### Phase 2 : Audit Lourd S√©lectif (Demain)

1. Ajouter m√©thode `stress_test_promoted()` pour HoF
2. Appliquer grilles 32√ó32, 64√ó64 seulement aux promus
3. Documenter r√©sultats dans `results/audit_promoted_v3.json`

### Phase 3 : Vectorisation (Semaine Prochaine, Si N√©cessaire)

1. Impl√©menter `step_ca_vectorized()` avec scipy
2. Benchmarker vs version Python pure
3. Int√©grer si gain > 10√ó

---

## üîí GARDE-FOUS ANTI-ARTEFACTS

En parall√®le du mode fast, **ajouter filtres anti-trivialit√©** :

### 1. Filtre Densit√© Finale

```python
def is_quasi_death_rule(final_density, threshold=0.05):
    """Rejette rules convergent vers vide."""
    return final_density < threshold

def is_saturation_rule(final_density, threshold=0.95):
    """Rejette rules saturant la grille."""
    return final_density > threshold
```

**Appliqu√© dans `_update_memory_and_hof()` avant promotion.**

### 2. Filtre Richness (Motifs Distincts)

```python
def compute_pattern_richness(grid, window_size=5):
    """Compte motifs 5√ó5 distincts dans grille finale."""
    h, w = grid.shape
    patterns = set()
    for i in range(0, h-window_size, window_size):
        for j in range(0, w-window_size, window_size):
            patch = grid[i:i+window_size, j:j+window_size]
            patterns.add(patch.tobytes())
    return len(patterns) / ((h//window_size) * (w//window_size))
```

**Seuil : richness >= 0.05 (au moins 5% diversit√© motifs).**

### 3. P√©nalit√© Score Composite

```python
# Dans compute_functional_score()
if final_density < 0.05 or final_density > 0.95:
    functional_score *= 0.1  # P√©nalit√© s√©v√®re

if pattern_richness < 0.05:
    functional_score *= 0.3  # P√©nalit√© mod√©r√©e
```

---

## üìä R√âSULTATS ATTENDUS

### Avant (Actuel)

- 150 it√©rations pr√©vues
- 1 it√©ration en 4h
- **Temps total estim√© : 600h (25 jours)** üî¥

### Apr√®s (Mode Fast + Filtres)

- 50 it√©rations (suffisant avec filtres)
- 1 it√©ration en ~3 minutes
- **Temps total : ~2.5h** ‚úÖ
- Audit lourd (32√ó32, 64√ó64) seulement sur top 5-10 promus

---

## üéØ CONCLUSION

**Cause racine :** M√©triques fonctionnelles trop lourdes (340 sims/candidat) en Python pur.

**Solution imm√©diate (ce soir) :** Mode fast (120 sims/candidat) + filtres anti-trivialit√©.

**Solution moyen terme (semaine prochaine) :** Vectorisation NumPy (gain 10-20√ó).

**R√©sultat :** AGI utilisable (50 it√©rations en 2-3h) avec qualit√© pr√©serv√©e.

---

**Impl√©mentation : scripts/fix_agi_performance_v3.py**

