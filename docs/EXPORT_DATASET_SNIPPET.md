# üì• Export Dataset - Enrichir le Training Set

**Usage**: Apr√®s avoir valid√© de nouvelles r√®gles avec Auto Memory Research

---

## üéØ Objectif

Exporter les r√©sultats valid√©s au format JSON pour enrichir `data/memory_rules_dataset.json` ou cr√©er un nouveau fichier de dataset.

---

## üìù Snippet Console

### 1. Exporter R√©sultats Valid√©s

**Sur**: `http://localhost:8001/experiments/auto-memory-research/`

```javascript
// Apr√®s avoir run le pipeline
await AutoMemoryResearch.runAll()

// R√©cup√©rer r√©sultats
const { mlSuggestions, validatedResults } = AutoMemoryResearch.getResults()

// Formater pour dataset
const samples = validatedResults.map(r => ({
  notation: r.notation,
  bornMask: [...Array(9)].map((_, i) => r.notation.match(/B([0-8]*)/)[1]?.includes(String(i)) ? 1 : 0),
  surviveMask: [...Array(9)].map((_, i) => r.notation.match(/S([0-8]*)/)[1]?.includes(String(i)) ? 1 : 0),
  isMemoryCandidate: r.isMemoryLike,
  avgRecall: r.avgRecall,
  maxCapacity: r.maxCapacity,
  mlProba: r.mlProba,
  source: 'auto_research_v1'
}))

// Afficher JSON
console.log(JSON.stringify(samples, null, 2))

// OU copier dans presse-papier
copy(JSON.stringify(samples, null, 2))
```

---

### 2. Format de Sortie

**Exemple**:
```json
[
  {
    "notation": "B01/S3",
    "bornMask": [1,1,0,0,0,0,0,0,0],
    "surviveMask": [0,0,0,1,0,0,0,0,0],
    "isMemoryCandidate": true,
    "avgRecall": 100,
    "maxCapacity": 10,
    "mlProba": 0.895,
    "source": "auto_research_v1"
  },
  {
    "notation": "B01/S4",
    "bornMask": [1,1,0,0,0,0,0,0,0],
    "surviveMask": [0,0,0,0,1,0,0,0,0],
    "isMemoryCandidate": true,
    "avgRecall": 99,
    "maxCapacity": 10,
    "mlProba": 0.887,
    "source": "auto_research_v1"
  }
]
```

---

### 3. Enrichir Dataset Existant

**M√©thode manuelle**:
1. Ex√©cuter snippet ci-dessus
2. Copier JSON dans presse-papier
3. Ouvrir `data/memory_rules_dataset.json`
4. Ajouter nouvelles entr√©es dans `rules` array
5. √âviter duplicatas (check notation)
6. Sauvegarder

**M√©thode automatique** (script Node):
```javascript
// scripts/enrich-dataset.js
import fs from 'fs';

const existing = JSON.parse(fs.readFileSync('data/memory_rules_dataset.json', 'utf8'));
const newSamples = [ /* coller r√©sultats ici */ ];

// Fusionner sans duplicatas
const notations = new Set(existing.rules.map(r => r.notation));
const toAdd = newSamples.filter(s => !notations.has(s.notation));

existing.rules.push(...toAdd);
existing.meta.version = '1.1';
existing.meta.date = new Date().toISOString().split('T')[0];

fs.writeFileSync('data/memory_rules_dataset_v1.1.json', JSON.stringify(existing, null, 2));
console.log(`Added ${toAdd.length} new rules`);
```

---

### 4. Re-Trainer le Predictor

**Apr√®s enrichissement**:
```javascript
// Sur rule-predictor page
location.reload()  // Force reload pour charger nouveau dataset

// V√©rifier
predictor.trainingStats.totalSamples
// Expected: 16 + N nouvelles r√®gles
```

---

## üîÑ Workflow Complet

### Cycle de D√©couverte & Enrichissement

```
1. Rule Predictor
   ‚Üì suggest candidates
   
2. Auto Memory Research
   ‚Üì validate top 10
   
3. Export r√©sultats valid√©s
   ‚Üì copy JSON
   
4. Enrichir dataset
   ‚Üì add to memory_rules_dataset.json
   
5. Re-train predictor
   ‚Üì reload rule-predictor page
   
6. Repeat (accuracy improves)
```

**It√©ration**: Chaque cycle ajoute 5-10 r√®gles valid√©es au dataset

---

## üìä Exemple Complet

### Sc√©nario: D√©couvrir 5 Nouvelles R√®gles

**1. Run Auto Research**:
```javascript
await AutoMemoryResearch.runAll()
```

**2. Export r√©sultats**:
```javascript
const { validatedResults } = AutoMemoryResearch.getResults()
const newRules = validatedResults.filter(r => r.isMemoryLike)
// Found 7 memory-capable rules

const samples = newRules.map(r => ({
  notation: r.notation,
  bornMask: [...Array(9)].map((_, i) => r.notation.match(/B([0-8]*)/)[1]?.includes(String(i)) ? 1 : 0),
  surviveMask: [...Array(9)].map((_, i) => r.notation.match(/S([0-8]*)/)[1]?.includes(String(i)) ? 1 : 0),
  isMemoryCandidate: true,
  avgRecall: r.avgRecall,
  maxCapacity: r.maxCapacity,
  mlProba: r.mlProba,
  source: 'auto_research_2025_11_08'
}))

copy(JSON.stringify(samples, null, 2))
```

**3. Enrichir dataset**:
- Ouvrir `data/memory_rules_dataset.json`
- Coller dans `rules` array
- Sauvegarder

**4. Re-train**:
- Reload `http://localhost:8001/experiments/rule-predictor/`
- V√©rifier `predictor.trainingStats.totalSamples` a augment√©

**5. V√©rifier accuracy**:
```javascript
const validation = predictor.validateKnown()
const accuracy = validation.filter(v => v.match).length / validation.length
console.log(`Accuracy: ${(accuracy * 100).toFixed(1)}%`)
```

---

## üéØ Impact Attendu

### Apr√®s 3-5 It√©rations

**Dataset**:
- 16 ‚Üí 40+ r√®gles
- Balance am√©lior√©
- Plus de diversity

**Accuracy**:
- Test: 75-100% ‚Üí 85-95% (plus stable)
- Confusion matrix plus √©quilibr√©e

**Suggestions**:
- Plus de true positives
- Moins de false positives
- Meilleures candidates

---

## ‚ö†Ô∏è Notes Importantes

### √âviter Duplicatas
Avant d'ajouter une r√®gle:
```javascript
const existing = JSON.parse(await fetch('../../data/memory_rules_dataset.json').then(r => r.text()))
const notations = existing.rules.map(r => r.notation)
const newRule = 'B01/S35'

if (notations.includes(newRule)) {
  console.warn('Rule already in dataset')
} else {
  console.log('OK to add')
}
```

### V√©rifier Qualit√©
Avant d'ajouter une r√®gle au dataset:
- avgRecall ‚â• 90% (crit√®re strict)
- maxCapacity ‚â• 10 (capacit√© suffisante)
- Test√©e avec protocole V1 complet

### Backup
```bash
# Avant modification
cp data/memory_rules_dataset.json data/memory_rules_dataset.json.backup
```

---

**Le snippet est pr√™t √† l'emploi.** Copy-paste dans la console pour exporter. üìã

